{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "gamornet_basics_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritraghsh09/GaMorNet/blob/master/tutorials/gamornet_tl_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNq4HgykhnJK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Google Colab Stuff\n",
        "\n",
        "Although this tutorial can be run on any machine which has GaMorNet installed, it's pretty handy to run this on Google Colab as you can easily use Colab's GPUs for this tutorial.\n",
        "\n",
        "Note that with the free version of Colab, you will only have access to a limited amount of memory. Thus, the number of images we use here for training/testing is very small. In reality, GaMorNet can handle hundreds of thousands of images. \n",
        "\n",
        "This first section is meant to be run only when following this tutorial in Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2wSGVu4ihqa",
        "colab_type": "text"
      },
      "source": [
        "### Make things Fast!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWvqtYGaijke",
        "colab_type": "text"
      },
      "source": [
        "Before we dive in, let's make sure we're using a GPU for this tutorial.  \n",
        "\n",
        "To do this, select \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> \"GPU\".\n",
        "\n",
        "The following snippet will verify that we have access to a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBNuljhhnyf",
        "colab_type": "code",
        "outputId": "4302e37b-e526-44f3-acae-219a534fdb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "# Suppressing TF warnings and info for a cleaner environ\n",
        "# Set this to 0,1 for info and warnings respectively.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        " \n",
        "# Magic telling Colab we want TF version ~=1.0\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "#Checking access to GPU\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  print('WARNING: GPU device not found.')\n",
        "else:\n",
        "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "SUCCESS: Found GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en4lZVmont-b",
        "colab_type": "text"
      },
      "source": [
        "### Install GaMorNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywr-x3csgQ5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q --upgrade gamornet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYsnfr7tj3D-",
        "colab_type": "code",
        "outputId": "addac1c0-896b-482e-dc7f-0dc406b182df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "##Checking which version of Tensorflow & GaMorNet is being used and whether the installation worked.\n",
        "import tensorflow as tf\n",
        "import gamornet\n",
        "print(tf.__version__)\n",
        "print(gamornet.__version__)\n",
        "from gamornet.keras_module import gamornet_train_keras, gamornet_tl_keras, gamornet_predict_keras\n",
        "from gamornet.tflearn_module import gamornet_train_tflearn, gamornet_tl_tflearn, gamornet_predict_tflearn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "0.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isCigPq4TiWi",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "All mentions of \"the paper\" in this tutorial, refer to [Ghosh et. al. (2020)](https://iopscience.iop.org/article/10.3847/1538-4357/ab8a47)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUDD3Qke7Ws_",
        "colab_type": "text"
      },
      "source": [
        "# Installing Libraries Needed for this Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBSCZfMM7WJV",
        "colab_type": "code",
        "outputId": "ce690ae0-955e-466a-a59f-53836969f4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip install matplotlib"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZdvFR_IgTmn",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning with GaMorNet\n",
        "\n",
        "GaMorNet models that have been trained before can easily be refined using new data. This is the basic idea behind transfer learning.  \n",
        "\n",
        "In this demonstration, we will start with simulation-trained SDSS model that we have released in the paper. Thereafter, we will perform transfer learning on this model. For this, we will use 90 real SDSS galaxies for the purposes of training and 10 real SDSS galaxies for validation. All these images are in the g-band and are part of the SDSS data-set used in the paper. \n",
        "\n",
        "The simulations that the base model was trained on contain disk + bulge components. As described in the paper, we also convolved these simulations with a representative PSF and added representative noise. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwzpM9YMUPqn",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the Data\n",
        "\n",
        "First, let's download the images that we are going to use to transfer learn. We will download these into the local filesystem from Yale Astronomy's FTP service, where these are hosted.\n",
        "\n",
        "We are going to download all the 100 images (90+10) as a single archive and then export it to a single folder called `training_imgs`. The iamges are in the FITS format and the names of the images are their SDSS Object IDs.\n",
        "\n",
        "We are also going to download the `gal_para.txt` file containing the ground-truth parameters for the above galaxies from [Simard et. al. (2011)](https://iopscience.iop.org/article/10.1088/0067-0049/196/1/11). Using thes bulge-to-total light ratio of each galaxy, we will determine the labels to be used during the transfer learning process. \n",
        "\n",
        "\n",
        "*Tip: The `%%bash` command lets Colab know that all the commands in this shell needs to be passed the local unix virtual environment.*\n",
        "\n",
        "*Tip: To view the files in use on Colab, click the folder icon on the left sidebar.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TThUIR8Y4lc",
        "colab_type": "code",
        "outputId": "e6c33dd3-6be5-4eea-835b-58c111eaecde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "#get zip and txt file from server\n",
        "wget ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/tl_images/tl_images.tar.gz\n",
        "wget ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/tl_images/gal_para.txt\n",
        "\n",
        "#Unzip the Archive\n",
        "tar -xvf tl_images.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./tl_images/587722953304440846-g.fits\n",
            "./tl_images/587722981750014081-g.fits\n",
            "./tl_images/587722982298026184-g.fits\n",
            "./tl_images/587722982812155970-g.fits\n",
            "./tl_images/587722982831161384-g.fits\n",
            "./tl_images/587722983365279858-g.fits\n",
            "./tl_images/587722983366721714-g.fits\n",
            "./tl_images/587722983369408606-g.fits\n",
            "./tl_images/587722983369408874-g.fits\n",
            "./tl_images/587722983897890856-g.fits\n",
            "./tl_images/587722983902806142-g.fits\n",
            "./tl_images/587722983908049268-g.fits\n",
            "./tl_images/587722983910932754-g.fits\n",
            "./tl_images/587722983912767673-g.fits\n",
            "./tl_images/587722984429322343-g.fits\n",
            "./tl_images/587722984438038680-g.fits\n",
            "./tl_images/587722984439545906-g.fits\n",
            "./tl_images/587722984440659987-g.fits\n",
            "./tl_images/587722984440791232-g.fits\n",
            "./tl_images/587722984443216081-g.fits\n",
            "./tl_images/587724197211144263-g.fits\n",
            "./tl_images/587724197741854834-g.fits\n",
            "./tl_images/587724197743362189-g.fits\n",
            "./tl_images/587724197745918122-g.fits\n",
            "./tl_images/587724197746704505-g.fits\n",
            "./tl_images/587724197747032159-g.fits\n",
            "./tl_images/587724198277415036-g.fits\n",
            "./tl_images/587724198278725806-g.fits\n",
            "./tl_images/587724198279053466-g.fits\n",
            "./tl_images/587724198279446731-g.fits\n",
            "./tl_images/587724198282461323-g.fits\n",
            "./tl_images/587724198282920103-g.fits\n",
            "./tl_images/587724198285279370-g.fits\n",
            "./tl_images/587724198816645382-g.fits\n",
            "./tl_images/587724198816841872-g.fits\n",
            "./tl_images/587724198822412473-g.fits\n",
            "./tl_images/587724199349846160-g.fits\n",
            "./tl_images/587724199352729859-g.fits\n",
            "./tl_images/587724199887044764-g.fits\n",
            "./tl_images/587724199889666153-g.fits\n",
            "./tl_images/587724199889797351-g.fits\n",
            "./tl_images/587724199894057120-g.fits\n",
            "./tl_images/587724231565574263-g.fits\n",
            "./tl_images/587724231567802508-g.fits\n",
            "./tl_images/587724232100675610-g.fits\n",
            "./tl_images/587724232102641838-g.fits\n",
            "./tl_images/587724232110375026-g.fits\n",
            "./tl_images/587724232638201976-g.fits\n",
            "./tl_images/587724232639512732-g.fits\n",
            "./tl_images/587724232640299135-g.fits\n",
            "./tl_images/587724232640823418-g.fits\n",
            "./tl_images/587724232641937529-g.fits\n",
            "./tl_images/587724232645017746-g.fits\n",
            "./tl_images/587724232645083176-g.fits\n",
            "./tl_images/587724233177759864-g.fits\n",
            "./tl_images/587724233178480762-g.fits\n",
            "./tl_images/587724233179791424-g.fits\n",
            "./tl_images/587724233712599147-g.fits\n",
            "./tl_images/587724233715023967-g.fits\n",
            "./tl_images/587724233715024035-g.fits\n",
            "./tl_images/587724233715089476-g.fits\n",
            "./tl_images/587724233717776443-g.fits\n",
            "./tl_images/587724233720004817-g.fits\n",
            "./tl_images/587724234249732319-g.fits\n",
            "./tl_images/587724234251436159-g.fits\n",
            "./tl_images/587724234251829389-g.fits\n",
            "./tl_images/587724240154853493-g.fits\n",
            "./tl_images/587724240687267890-g.fits\n",
            "./tl_images/587724240690282547-g.fits\n",
            "./tl_images/587724240694673502-g.fits\n",
            "./tl_images/587724241226170380-g.fits\n",
            "./tl_images/587724241228726404-g.fits\n",
            "./tl_images/587724241232462000-g.fits\n",
            "./tl_images/587724242300895371-g.fits\n",
            "./tl_images/587724242308038811-g.fits\n",
            "./tl_images/587724648179433616-g.fits\n",
            "./tl_images/587724648185135182-g.fits\n",
            "./tl_images/587724648188674155-g.fits\n",
            "./tl_images/587724648191098979-g.fits\n",
            "./tl_images/587724648723710103-g.fits\n",
            "./tl_images/587724648726331503-g.fits\n",
            "./tl_images/587724649255469071-g.fits\n",
            "./tl_images/587724649790111843-g.fits\n",
            "./tl_images/587724650327113903-g.fits\n",
            "./tl_images/587724650329145594-g.fits\n",
            "./tl_images/587724650329276573-g.fits\n",
            "./tl_images/587724650864050209-g.fits\n",
            "./tl_images/587724650864181415-g.fits\n",
            "./tl_images/587724650864574562-g.fits\n",
            "./tl_images/587724650864574666-g.fits\n",
            "./tl_images/587725039016280152-g.fits\n",
            "./tl_images/587725039019491473-g.fits\n",
            "./tl_images/587725039021916298-g.fits\n",
            "./tl_images/587725039026307236-g.fits\n",
            "./tl_images/587725039555248209-g.fits\n",
            "./tl_images/587725039560491033-g.fits\n",
            "./tl_images/587725039562850481-g.fits\n",
            "./tl_images/587725040093036670-g.fits\n",
            "./tl_images/587725040093298792-g.fits\n",
            "./tl_images/587725040626106522-g.fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 00:14:11--  ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/tl_images/tl_images.tar.gz\n",
            "           => ‘tl_images.tar.gz.1’\n",
            "Resolving ftp.astro.yale.edu (ftp.astro.yale.edu)... 128.36.139.12\n",
            "Connecting to ftp.astro.yale.edu (ftp.astro.yale.edu)|128.36.139.12|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /pub/aghosh/gamornet_tutorial_files/tl_images ... done.\n",
            "==> SIZE tl_images.tar.gz ... 3709327\n",
            "==> PASV ... done.    ==> RETR tl_images.tar.gz ... done.\n",
            "Length: 3709327 (3.5M) (unauthoritative)\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  1% 52.7K 68s\n",
            "    50K .......... .......... .......... .......... ..........  2% 44.1K 73s\n",
            "   100K .......... .......... .......... .......... ..........  4% 73.6K 64s\n",
            "   150K .......... .......... .......... .......... ..........  5% 67.8K 60s\n",
            "   200K .......... .......... .......... .......... ..........  6% 59.4K 59s\n",
            "   250K .......... .......... .......... .......... ..........  8% 67.1K 56s\n",
            "   300K .......... .......... .......... .......... ..........  9% 55.4K 56s\n",
            "   350K .......... .......... .......... .......... .......... 11% 44.1K 57s\n",
            "   400K .......... .......... .......... .......... .......... 12% 44.7K 58s\n",
            "   450K .......... .......... .......... .......... .......... 13% 36.6K 60s\n",
            "   500K .......... .......... .......... .......... .......... 15% 31.6K 62s\n",
            "   550K .......... .......... .......... .......... .......... 16% 35.4K 63s\n",
            "   600K .......... .......... .......... .......... .......... 17% 38.5K 64s\n",
            "   650K .......... .......... .......... .......... .......... 19% 46.7K 62s\n",
            "   700K .......... .......... .......... .......... .......... 20% 49.0K 61s\n",
            "   750K .......... .......... .......... .......... .......... 22% 67.9K 59s\n",
            "   800K .......... .......... .......... .......... .......... 23% 73.5K 57s\n",
            "   850K .......... .......... .......... .......... .......... 24% 73.5K 55s\n",
            "   900K .......... .......... .......... .......... .......... 26% 73.5K 53s\n",
            "   950K .......... .......... .......... .......... .......... 27% 69.3K 51s\n",
            "  1000K .......... .......... .......... .......... .......... 28% 36.8K 51s\n",
            "  1050K .......... .......... .......... .......... .......... 30% 34.0K 51s\n",
            "  1100K .......... .......... .......... .......... .......... 31% 34.0K 51s\n",
            "  1150K .......... .......... .......... .......... .......... 33% 44.2K 50s\n",
            "  1200K .......... .......... .......... .......... .......... 34% 44.1K 49s\n",
            "  1250K .......... .......... .......... .......... .......... 35% 55.1K 48s\n",
            "  1300K .......... .......... .......... .......... .......... 37% 44.1K 47s\n",
            "  1350K .......... .......... .......... .......... .......... 38% 73.7K 46s\n",
            "  1400K .......... .......... .......... .......... .......... 40% 44.2K 45s\n",
            "  1450K .......... .......... .......... .......... .......... 41% 44.2K 44s\n",
            "  1500K .......... .......... .......... .......... .......... 42% 44.3K 43s\n",
            "  1550K .......... .......... .......... .......... .......... 44% 42.0K 42s\n",
            "  1600K .......... .......... .......... .......... .......... 45% 44.2K 41s\n",
            "  1650K .......... .......... .......... .......... .......... 46% 36.8K 41s\n",
            "  1700K .......... .......... .......... .......... .......... 48% 31.6K 40s\n",
            "  1750K .......... .......... .......... .......... .......... 49% 44.1K 39s\n",
            "  1800K .......... .......... .......... .......... .......... 51% 31.6K 38s\n",
            "  1850K .......... .......... .......... .......... .......... 52% 37.0K 38s\n",
            "  1900K .......... .......... .......... .......... .......... 53% 27.6K 37s\n",
            "  1950K .......... .......... .......... .......... .......... 55% 26.8K 37s\n",
            "  2000K .......... .......... .......... .......... .......... 56% 36.8K 36s\n",
            "  2050K .......... .......... .......... .......... .......... 57% 35.4K 35s\n",
            "  2100K .......... .......... .......... .......... .......... 59% 46.7K 34s\n",
            "  2150K .......... .......... .......... .......... .......... 60% 35.4K 33s\n",
            "  2200K .......... .......... .......... .......... .......... 62% 28.7K 32s\n",
            "  2250K .......... .......... .......... .......... .......... 63% 36.9K 31s\n",
            "  2300K .......... .......... .......... .......... .......... 64% 51.2K 30s\n",
            "  2350K .......... .......... .......... .......... .......... 66% 36.9K 28s\n",
            "  2400K .......... .......... .......... .......... .......... 67% 36.9K 27s\n",
            "  2450K .......... .......... .......... .......... .......... 69% 44.9K 26s\n",
            "  2500K .......... .......... .......... .......... .......... 70% 72.1K 25s\n",
            "  2550K .......... .......... .......... .......... .......... 71% 44.3K 24s\n",
            "  2600K .......... .......... .......... .......... .......... 73% 55.4K 22s\n",
            "  2650K .......... .......... .......... .......... .......... 74% 35.3K 21s\n",
            "  2700K .......... .......... .......... .......... .......... 75% 46.7K 20s\n",
            "  2750K .......... .......... .......... .......... .......... 77% 44.3K 19s\n",
            "  2800K .......... .......... .......... .......... .......... 78% 55.5K 18s\n",
            "  2850K .......... .......... .......... .......... .......... 80% 67.3K 17s\n",
            "  2900K .......... .......... .......... .......... .......... 81% 59.7K 15s\n",
            "  2950K .......... .......... .......... .......... .......... 82% 62.2K 14s\n",
            "  3000K .......... .......... .......... .......... .......... 84% 58.9K 13s\n",
            "  3050K .......... .......... .......... .......... .......... 85% 52.1K 12s\n",
            "  3100K .......... .......... .......... .......... .......... 86% 59.4K 11s\n",
            "  3150K .......... .......... .......... .......... .......... 88% 55.2K 9s\n",
            "  3200K .......... .......... .......... .......... .......... 89% 73.6K 8s\n",
            "  3250K .......... .......... .......... .......... .......... 91% 73.7K 7s\n",
            "  3300K .......... .......... .......... .......... .......... 92% 68.1K 6s\n",
            "  3350K .......... .......... .......... .......... .......... 93% 64.1K 5s\n",
            "  3400K .......... .......... .......... .......... .......... 95% 73.8K 4s\n",
            "  3450K .......... .......... .......... .......... .......... 96% 73.8K 3s\n",
            "  3500K .......... .......... .......... .......... .......... 98% 51.1K 2s\n",
            "  3550K .......... .......... .......... .......... .......... 99% 44.3K 0s\n",
            "  3600K .......... .......... ..                              100% 30.6K=79s\n",
            "\n",
            "2020-06-15 00:15:32 (46.1 KB/s) - ‘tl_images.tar.gz.1’ saved [3709327]\n",
            "\n",
            "--2020-06-15 00:15:32--  ftp://ftp.astro.yale.edu/pub/aghosh/gamornet_tutorial_files/tl_images/gal_para.txt\n",
            "           => ‘gal_para.txt.1’\n",
            "Resolving ftp.astro.yale.edu (ftp.astro.yale.edu)... 128.36.139.12\n",
            "Connecting to ftp.astro.yale.edu (ftp.astro.yale.edu)|128.36.139.12|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /pub/aghosh/gamornet_tutorial_files/tl_images ... done.\n",
            "==> SIZE gal_para.txt ... 7266\n",
            "==> PASV ... done.    ==> RETR gal_para.txt ... done.\n",
            "Length: 7266 (7.1K) (unauthoritative)\n",
            "\n",
            "     0K .......                                               100%  195M=0s\n",
            "\n",
            "2020-06-15 00:15:35 (195 MB/s) - ‘gal_para.txt.1’ saved [7266]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAwLO4hk-C3Q",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "In this section, we will generate the training and validation image arrays as well as the corresponding labels to be used during the transfer learning process.\n",
        "\n",
        "\n",
        "First, lets read in the `.txt` file, which will help us to figure out the label for each galaxy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex46Lm2c_3I8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pylab as plt\n",
        "\n",
        "#Let's read in the sim_para.txt file \n",
        "gal_para = plt.genfromtxt(\"./gal_para.txt\",names=True,dtype=None,encoding=None)\n",
        " \n",
        "#The file has a column called \"bt_g\" which is the bulge to total light ratio\n",
        "#for each galaxy as measured by Simard et. al. 2011. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pDautJ1CWTy",
        "colab_type": "text"
      },
      "source": [
        "Next, let's define two convenience functions, which will assist us in creating the image and label arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWcg5PBi-RSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convenience Function to get and return images as numpy arrays\n",
        "\n",
        "def image_handler(i):\n",
        "  return np.reshape(fits.getdata(\"./tl_images/\"+gal_para[\"file_name\"][i],\n",
        "                                 memmap=False),newshape=(167,167,1)) \n",
        "  #We use the reshape command just to add the extra 3rd dimension. The image is \n",
        "  #originally 167*167. So, in essence no re-sizing is taking place in the X or Y\n",
        "  #directions.\n",
        "\n",
        "\n",
        "# Convenience Function to get and return the training labels of each galaxy\n",
        "# in the one-hot encoding format. i.e. disk-dominated galaxies will be represented\n",
        "# by the array [1,0,0], bulge-dominated by [0,0,1] and indeterminate by [0,1,0]\n",
        "\n",
        "def label_handler(i):\n",
        "  \n",
        "  target_vect = [0]*3\n",
        "\n",
        "  if gal_para[\"bt_g\"][i] < 0.45: #disk-dminated\n",
        "    target_vect[0] = 1\n",
        "\n",
        "  elif 0.45 <= gal_para[\"bt_g\"][i] <= 0.55: #indeterminate\n",
        "    target_vect[1] = 1\n",
        "\n",
        "  else: #bulge-dominated\n",
        "    target_vect[2] = 1\n",
        "\n",
        "  return target_vect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQesQrpK-Pz2",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to use the first 90 images to create the training set and the last 10 to create the validation set. We are multi-threading the process below -- although this is an absolute overkill for 100 images, it's very handy while dealing with large numbers of images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_SCF8wWDiSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "from astropy.io import fits\n",
        "\n",
        "NUM_THREADS = 2\n",
        "\n",
        "pl = Pool(NUM_THREADS)\n",
        "training_imgs = np.array(pl.map(image_handler,range(0,90)))\n",
        "training_labels = np.array(pl.map(label_handler,range(0,90)))\n",
        "\n",
        "valdiation_imgs = np.array(pl.map(image_handler,range(90,100)))\n",
        "validation_labels = np.array(pl.map(label_handler,range(90,100)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw-l4YKMbLdK",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning on GaMorNet using Keras\n",
        "\n",
        "Now, we will be using the images and the labels generated above to transfer learn on GaMorNet. \n",
        "\n",
        "The main goal of transfer learning is to refine a previously trained model. Thus, as a starting point, we will be using the model trained on SDSS simulations in the paper. \n",
        "\n",
        "One of the other primary decisions to take during transfer learning is \n",
        "\n",
        "* whether you want to train all the layers or whether you want to freeze some of the them. The `trainable_bools` parameter controls this. \n",
        "\n",
        "* whether you want to load all the layers from the previously trained model or whether you want to initialize some of the layers from scratch. The `load_layers_bools` parameter controls this. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBTLm74QbMnG",
        "colab_type": "code",
        "outputId": "99e850bd-043f-4911-b68a-80c08f16f028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from gamornet.keras_module import gamornet_tl_keras\n",
        "\n",
        "model = gamornet_tl_keras(training_imgs,training_labels,valdiation_imgs,validation_labels, \n",
        "                          input_shape='SDSS', load_layers_bools='load_bools_SDSS', \n",
        "                          trainable_bools='train_bools_SDSS', \n",
        "                          model_load_path='SDSS_sim', epochs=20, checkpoint_freq=10, \n",
        "                          batch_size=64, lr=0.00001, loss='categorical_crossentropy')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Fetching SDSS Sim Trained Weigths.....\n",
            "Loading Layer0 from previous model.\n",
            "Loading Layer3 from previous model.\n",
            "Loading Layer6 from previous model.\n",
            "Loading Layer7 from previous model.\n",
            "Loading Layer8 from previous model.\n",
            "Initializing Layer12 from scratch\n",
            "Initializing Layer14 from scratch\n",
            "Initializing Layer16 from scratch\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Train on 90 samples, validate on 10 samples\n",
            "Epoch 1/20\n",
            "90/90 [==============================] - 9s 103ms/step - loss: 1.4707 - accuracy: 0.2333 - val_loss: 1.2592 - val_accuracy: 0.3000\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.3564 - accuracy: 0.3556 - val_loss: 1.2427 - val_accuracy: 0.3000\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.3181 - accuracy: 0.3556 - val_loss: 1.2221 - val_accuracy: 0.3000\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.3886 - accuracy: 0.3778 - val_loss: 1.1985 - val_accuracy: 0.3000\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.3547 - accuracy: 0.3111 - val_loss: 1.1727 - val_accuracy: 0.3000\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.2349 - accuracy: 0.4778 - val_loss: 1.1480 - val_accuracy: 0.3000\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.2011 - accuracy: 0.4667 - val_loss: 1.1252 - val_accuracy: 0.3000\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.2499 - accuracy: 0.4667 - val_loss: 1.1037 - val_accuracy: 0.4000\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.2234 - accuracy: 0.4556 - val_loss: 1.0836 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 1.0241 - accuracy: 0.5778 - val_loss: 1.0665 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00010: saving model to ./model_10.hdf5\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.9269 - accuracy: 0.5667 - val_loss: 1.0518 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.9691 - accuracy: 0.5444 - val_loss: 1.0399 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.8894 - accuracy: 0.6333 - val_loss: 1.0298 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.9981 - accuracy: 0.5667 - val_loss: 1.0216 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.7651 - accuracy: 0.6556 - val_loss: 1.0149 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.8449 - accuracy: 0.6444 - val_loss: 1.0090 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.7744 - accuracy: 0.7333 - val_loss: 1.0031 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.8122 - accuracy: 0.6556 - val_loss: 0.9979 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.8374 - accuracy: 0.6889 - val_loss: 0.9934 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "90/90 [==============================] - 0s 1ms/step - loss: 0.7965 - accuracy: 0.7111 - val_loss: 0.9886 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00020: saving model to ./model_20.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbzt1qHGG7ug",
        "colab_type": "text"
      },
      "source": [
        "In the output above, the `accuracy` and `loss` refer to the metrics calculated on the training set at the end of each epoch while `val_loss` and `val_accuracy` refer to the metrics calculated on the validation data. \n",
        "\n",
        "The above command trains the model using the images we prepared for 20 epochs using a learning rate of 0.00001 and a categorical cross-entropy loss function. The `checkpoint_freq = 10` parameter also ensures that every 10 epochs, a snapshot of the model is saved. These models are named as `model_x.hdf5` where x refers to the epoch at which the model was saved. The `input_shape` parameter specifies the shape of the input images. Setting this to `SDSS` automatically sets the value to `(167,167,1)`.\n",
        "\n",
        "---\n",
        "\n",
        "The value of the `load_layers_bools` parameter should be an array of eight bools corresponding to layer numbers 2, 5, 8, 9, 10, 13, 15, 17 of GaMorNet [(Schematic Diagram)](https://github.com/aritraghsh09/GaMorNet/blob/master/docs/source/images/gamornet_schematic_coloured.png). Setting an array element to `True` implies that the specific layer is loaded from the previous model, while setting an element to `False` implies that the specific layer is initialized from scratch. `load_layers_bools = 'load_bools_SDSS'` sets up the configuration we used in the paper for the SDSS data. \n",
        "\n",
        "The value of the `trainable_bools` parameter should be an array of eight bools again corresponding to layer numbers 2, 5, 8, 9, 10, 13, 15, 17 of GaMorNet [(Schematic Diagram)](https://github.com/aritraghsh09/GaMorNet/blob/master/docs/source/images/gamornet_schematic_coloured.png). Setting an array element to `True` means that wieghts and baises of that layer will be tuned during transfer learning. Setting it to `False` means that they would be kept frozen.`trainable_bools = 'train_bools_SDSS'` sets up the configuration that we used in the paper for SDSS data. \n",
        "\n",
        "For an explanation of the different input parameters of `gamornet_tl_keras`, pelase have a look at the [API documentation](https://gamornet.readthedocs.io/en/latest/api_docs.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhQgRdXJtwE",
        "colab_type": "text"
      },
      "source": [
        "Thus, you can easily perform transfer learning using GaMorNet!! You can have a look at the trained model's structure using the command below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fs124brJpVI",
        "colab_type": "code",
        "outputId": "65466ef6-2a32-43d0-ad77-4520e1e00cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 42, 42, 96)        11712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 96)        0         \n",
            "_________________________________________________________________\n",
            "local_response_normalization (None, 21, 21, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 21, 21, 256)       614656    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "local_response_normalization (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 384)       885120    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 384)       1327488   \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 256)       884992    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "local_response_normalization (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              37752832  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 58,270,403\n",
            "Trainable params: 58,270,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkVOkDibLMmD",
        "colab_type": "text"
      },
      "source": [
        "**Important:**\n",
        "The above process also generates a `metrics.csv` file, which contains the loss and accuracy calculated on the validation as well as the training data. \n",
        "\n",
        "We highly recommend using the data in this file to check how the loss and accuracies vary with transfer learning. This is extremely helpful in judging whether the model was trained properly and sufficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oNUoKZkonnlh"
      },
      "source": [
        "# Transfer Learning on GaMorNet using TFLearn\n",
        "\n",
        "Now, we will be using the images and the labels generated above to transfer learn on GaMorNet. \n",
        "\n",
        "The main goal of transfer learning is to refine a previously trained model. Thus, as a starting point, we will be using the model trained on SDSS simulations in the paper. \n",
        "\n",
        "One of the other primary decisions to take during transfer learning is \n",
        "\n",
        "* whether you want to train all the layers or whether you want to freeze some of the them. The `trainable_bools` parameter controls this. \n",
        "\n",
        "* whether you want to load all the layers from the previously trained model or whether you want to initialize some of the layers from scratch. The `load_layers_bools` parameter controls this. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7399de31-a5e3-49c8-be40-81e6d8223436",
        "id": "xbSFipKwnnlj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from gamornet.tflearn_module import gamornet_tl_tflearn\n",
        "\n",
        "model = gamornet_tl_tflearn(training_imgs,training_labels,valdiation_imgs,validation_labels, \n",
        "                          input_shape='SDSS', load_layers_bools='load_bools_SDSS', \n",
        "                          trainable_bools='train_bools_SDSS', \n",
        "                          model_load_path='SDSS_sim', epochs=20, max_checkpoints=2, \n",
        "                          batch_size=64, lr=0.00001, loss='categorical_crossentropy',\n",
        "                          clear_session=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 1405632  | total loss: \u001b[1m\u001b[32m0.34465\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Momentum | epoch: 020 | loss: 0.34465 - acc: 0.8652 -- iter: 64/90\n",
            "Training Step: 1405633  | total loss: \u001b[1m\u001b[32m0.35792\u001b[0m\u001b[0m | time: 1.139s\n",
            "| Momentum | epoch: 020 | loss: 0.35792 - acc: 0.8633 | val_loss: 0.67395 - val_acc: 0.7000 -- iter: 90/90\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v_DHvyj7nnlp"
      },
      "source": [
        "The above command trains a model using the images we prepared for 20 epochs using a learning rate of 0.00001 and a categorical cross-entropy loss function. The `max_checkpoints = 2` parameter ensures that the latest 2 snapshots of the epochs will always be saved during training. Three files are saved for each snapshot and the naming format of the checkpoints is `check-x.data`,`check-x.index`,`check-x.meta` where x refers to the step number at which the model was saved. The `input_shape` parameter specifies the shape of the input images. Setting this to `SDSS` automatically sets the value to `(167,167,1)`. \n",
        "\n",
        "In the output above, the `acc` and `loss` refer to the accuracy and loss calculated on the training set at the end of each epoch while `val_loss` and `val_acc` refer to the metrics calculated on the validation data. \n",
        "\n",
        "---\n",
        "\n",
        "The value of the `load_layers_bools` parameter should be an array of eight bools corresponding to layer numbers 2, 5, 8, 9, 10, 13, 15, 17 of GaMorNet [(Schematic Diagram)](https://github.com/aritraghsh09/GaMorNet/blob/master/docs/source/images/gamornet_schematic_coloured.png). Setting an array element to `True` implies that the specific layer is loaded from the previous model, while setting an element to `False` implies that the specific layer is initialized from scratch. `load_layers_bools = 'load_bools_SDSS'` sets up the configuration we used in the paper for the SDSS data. \n",
        "\n",
        "The value of the `trainable_bools` parameter should be an array of eight bools again corresponding to layer numbers 2, 5, 8, 9, 10, 13, 15, 17 of GaMorNet [(Schematic Diagram)](https://github.com/aritraghsh09/GaMorNet/blob/master/docs/source/images/gamornet_schematic_coloured.png). Setting an array element to `True` means that wieghts and baises of that layer will be tuned during transfer learning. Setting it to `False` means that they would be kept frozen.`trainable_bools = 'train_bools_SDSS'` sets up the configuration that we used in the paper for SDSS data.\n",
        "\n",
        "--- \n",
        "\n",
        "The `clear_session = True` parameter value instructs GaMorNet to clear the TensorFlow graphs created earlier. We highly recommend setting `clear_session` to `True` in notebooks while using the `tflearn_module` as otherwise it might fail. \n",
        "\n",
        "For an explanation of the different input parameters of `gamornet_tl_tflearn`, pelase have a look at the [API documentation](https://gamornet.readthedocs.io/en/latest/api_docs.html).\n",
        "\n",
        "Thus, you can easily perform transfer learning using GaMorNet!! \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WkkqhRiynnlq"
      },
      "source": [
        "**Tip:** Unlike with the keras module, the tflearn module doesn't automatically save the metrics. Instead you have to redirect the Python output generated to a file in order to keep track of the metrics. \n",
        "\n",
        "When running a python script, this can be done simply using `python script.py > out.txt`. This will save all the screen output in `out.txt`.\n",
        "\n",
        "Thereafter the following snippet of Python Code can easily search for the relevant metrics in the screen output file. \n",
        "\n",
        "```python\n",
        "###################################\n",
        "# accParser.py\n",
        "#\n",
        "# Takes tflearn screen output and extracts loss, acc and val_acc every epoch for visualization\n",
        "####################################\n",
        "import sys\n",
        "\n",
        "if (len(sys.argv) != 2):\n",
        "        print \"Exiting Program....\\nUsage: python accParser.py /path/to/screen/output\"\n",
        "\n",
        "\n",
        "dataPath = sys.argv[1] #the first argument is the path to the screen grab of the TF Learn run\n",
        "\n",
        "dataFile = open(dataPath, 'r')\n",
        "outFile = open(dataPath[:-6] + 'out.txt', 'w')\n",
        "\n",
        "outFile.write(\"epoch loss acc val_acc\\n\")\n",
        "resultLines = dataFile.readlines()\n",
        "\n",
        "for line in resultLines:\n",
        "        if 'val_acc' in line:\n",
        "                words = line.split()\n",
        "\n",
        "                #validation step\n",
        "                if words[-2:-1] != ['iter:']:\n",
        "                        print \"Something doesn't look right. Skipping an occurene of val_acc\"\n",
        "                        continue\n",
        "\n",
        "                outFile.write(words[words.index(\"epoch:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"loss:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"acc:\")+1] + \" \")\n",
        "                outFile.write(words[words.index(\"val_acc:\")+1] + \"\\n\")\n",
        "\n",
        "dataFile.close()\n",
        "outFile.close()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CI4QJAROoGo",
        "colab_type": "text"
      },
      "source": [
        "**Important:** We highly recommend checking how the loss and accuracies vary with transfer learning. This is extremely helpful in judging whether the model was trained properly and sufficiently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Trd52HQV7xU",
        "colab_type": "text"
      },
      "source": [
        "# Summary & Takeaways\n",
        "\n",
        "* `gamornet_tl_keras` and `gamornet_tl_tflearn` are the two functions that can be used to perform transfer learning (i.e. fine-tune previously trained GaMorNet models).  \n",
        "\n",
        "* For understanding the differences between the Keras and TFLearn modules, please refer to the [PDR Handbook](https://gamornet.readthedocs.io/en/latest/usage_guide.html). \n",
        "\n",
        "* The [PDR Handbook](https://gamornet.readthedocs.io/en/latest/usage_guide.html) also contains advice about which model is the ideal candidate for starting the transfer learning process. "
      ]
    }
  ]
}